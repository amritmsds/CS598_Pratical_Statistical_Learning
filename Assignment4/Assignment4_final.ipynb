{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team member and Contributions\n",
    "> Shu Xu (shuxu3@illinois.edu): Part II/III\n",
    "\n",
    "> Yan Han (yanhan4@illinois.edu): Part I\n",
    "\n",
    "> Amrit Kumar(amritk2@illinois.edu): Part II\n",
    "\n",
    "**We finish this notebook together.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I:  Gaussian Mixtures\n",
    "\n",
    "### Objective\n",
    "Implement the EM algorithm **from scratch** for a $p$-dimensional Gaussian mixture model \n",
    "with $G$ components: \n",
    "$$\n",
    "\\sum_{k=1}^G p_k \\cdot \\textsf{N}(x; \\mu_k, \\Sigma).\n",
    "$$\n",
    "### Requirements\n",
    "\n",
    "Your implementation should consists of **four** functions. \n",
    "\n",
    "- **`Estep`** function: This function should return an $n$-by-$G$ matrix, where the $(i,j)$th entry represents the conditional probability $P(Z_i = k \\mid x_i)$. Here $i$ ranges from 1 to $n$ and $k$ ranges from $1$ to $G$.\n",
    "\n",
    "- **`Mstep`** function: This function should return the updated parameters for the Gaussian mixture model.\n",
    "\n",
    "- **`loglik`** function: This function computes the log-likelihood of the data given the parameters.\n",
    "\n",
    "- **`myEM`** function (main function): Inside this function, you can call the `Estep`, `Mstep`, and `loglik` functions. The function should take the following inputs and return the estimated parameters and log-likelihood:     \n",
    "\n",
    "  - **Input**: \n",
    "    - data: The dataset.\n",
    "    - $G$: The number of components.\n",
    "    - Initial parameters.\n",
    "    - `itmax`: The number of iterations.\n",
    "  - **Output**: \n",
    "    - `prob`: A $G$-dimensional probability vector $(p_1, \\dots, p_G)$\n",
    "    - `mean`: A $p$-by-$G$ matrix with the $k$-th column being $\\mu_k$, the $p$-dimensional mean for the $k$-th Gaussian component. \n",
    "    - `Sigma`: A $p$-by-$p$ covariance matrix $\\Sigma$ shared by all $G$ components; \n",
    "    - `loglik`: A number equal to $\\sum_{i=1}^n \\log \\Big [ \\sum_{k=1}^G p_k \\cdot \\textsf{N}(x; \\mu_k, \\Sigma) \\Big ].$\n",
    "\n",
    "**Implementation Guidelines:**\n",
    "\n",
    "  - Avoid explicit loops over the sample size $n$.\n",
    "  - You are allowed to use loops over the number of components $G$, although you can avoid all loops. \n",
    "  - You are not allowed to use pre-existing functions or packages for evaluating normal densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estep(data, G, prob, mu, Sigma):\n",
    "    g = np.zeros((len(data), G))\n",
    "    U, D, _ = np.linalg.svd(Sigma)\n",
    "    D_tilda = np.diag(1 / np.sqrt(D))\n",
    "    x_tilda = D_tilda.dot(U.T.dot(data.T)).T # n by dim\n",
    "    mu_tilda = D_tilda.dot(U.T.dot(mu)).T # G by dim\n",
    "    for i in range(G):\n",
    "        g[:, i] = np.sum(((x_tilda - mu_tilda[i])*(x_tilda - mu_tilda[i])), axis=1)\n",
    "        g[:, i] = np.exp(-0.5 * g[:, i]) * prob[i]\n",
    "    g /= np.sqrt((2*math.pi) ** ndim * np.linalg.det(Sigma))\n",
    "    g /= np.sum(g, axis=1, keepdims=True)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mstep(data, g):\n",
    "    prob = np.mean(g, axis=0)\n",
    "    mu = np.zeros((ndim, G))\n",
    "    Sigma = np.zeros((ndim, ndim))\n",
    "    for i in range(G):\n",
    "        mu[:, i] = np.sum(np.multiply(g[:, i:i+1], data), axis=0) / np.sum(g[:, i])\n",
    "        temp_data = data - mu[:, i]\n",
    "        temp_data1 = np.multiply(g[:, i:i+1], temp_data)\n",
    "        Sigma += temp_data.T.dot(temp_data1)\n",
    "    Sigma /= n\n",
    "    return prob, mu, Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglik(data, prob, mu, Sigma):\n",
    "    g = np.zeros((len(data), G))\n",
    "    U, D, _ = np.linalg.svd(Sigma)\n",
    "    D_tilda = np.diag(1 / np.sqrt(D))\n",
    "    x_tilda = D_tilda.dot(U.T.dot(data.T)).T # n by dim\n",
    "    mu_tilda = D_tilda.dot(U.T.dot(mu)).T\n",
    "    for i in range(G):\n",
    "        g[:, i] = np.sum(((x_tilda - mu_tilda[i])*(x_tilda - mu_tilda[i])), axis=1)\n",
    "        g[:, i] = np.exp(-0.5 * g[:, i]) * prob[i]\n",
    "    g /= np.sqrt((2*math.pi) ** ndim * np.linalg.det(Sigma))\n",
    "    llh = np.sum(np.log(np.sum(g, axis=1)), axis=0)\n",
    "    return llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myEM(data, G, prob, mu, Sigma, itmax=20):\n",
    "    for i in range(itmax):\n",
    "        g = Estep(data, G, prob, mu, Sigma)\n",
    "        prob, mu, Sigma = Mstep(data, g)\n",
    "    return prob, mu, Sigma, loglik(data, prob, mu, Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Test your code with the provided dataset,  [[faithful.dat](https://liangfgithub.github.io/Data/faithful.dat)], with both $G=2$ and $G=3$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"faithful.dat\", sep=\"\\s+\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the case when $G=2$**, set your initial values as follows:\n",
    "\n",
    "- $p_1 = 10/n$, $p_2 = 1 - p_1$.\n",
    "- $\\mu_1$ =  the mean of the first 10 samples; $\\mu_2$ = the mean of the remaining samples.\n",
    "- Calculate $\\Sigma$ as  \n",
    "$$\n",
    "\\frac{1}{n} \\Big [ \\sum_{i=1}^{10} (x_i- \\mu_1)(x_i- \\mu_1)^t + \\sum_{i=11}^n (x_i- \\mu_2)(x_i- \\mu_2)^t \\Big].\n",
    "$$\n",
    "Here $x_i - \\mu_i$ is a 2-by-1 vector, so the resulting $\\Sigma$ matrix is a 2-by-2 matrix. \n",
    "\n",
    "Run your EM implementation with **20** iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "G = 2\n",
    "ndim = 2\n",
    "prob = np.array([10/n, 1-10/n])\n",
    "mu = np.array([np.mean(data[:10], axis=0), np.mean(data[10:], axis=0)])\n",
    "mu = mu.T\n",
    "Sigma = np.zeros((2,2))\n",
    "temp_data1 = np.array(data[:10]-mu[:, 0])\n",
    "temp_data2 = np.array(data[10:]-mu[:, 1])\n",
    "Sigma[0, 0] = np.sum(np.multiply(temp_data1[:,0], temp_data1[:,0])) + np.sum(np.multiply(temp_data2[:,0], temp_data2[:,0]))\n",
    "Sigma[0, 1] = Sigma[1, 0] = np.sum(np.multiply(temp_data1[:,0],temp_data1[:,1]))+np.sum(np.multiply(temp_data2[:,0],temp_data2[:,1]))\n",
    "Sigma[1, 1] = np.sum(np.multiply(temp_data1[:,1], temp_data1[:,1])) + np.sum(np.multiply(temp_data2[:,1], temp_data2[:,1]))\n",
    "Sigma /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob, new_mu, new_Sigma, llh = myEM(data, G, prob, mu, Sigma, itmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob\n",
      "[0.04297883 0.95702117]\n",
      "mean\n",
      "[[ 3.49564188  3.48743016]\n",
      " [76.79789154 70.63205853]]\n",
      "Sigma\n",
      "           eruptions     waiting\n",
      "eruptions   1.297936   13.924336\n",
      "waiting    13.924336  182.580092\n",
      "loglik\n",
      "-1289.5693549424138\n"
     ]
    }
   ],
   "source": [
    "print(\"prob\")\n",
    "print(new_prob)\n",
    "print(\"mean\")\n",
    "print(new_mu)\n",
    "print(\"Sigma\")\n",
    "print(new_Sigma)\n",
    "print(\"loglik\")\n",
    "print(llh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the case when $G=3$**, set your initial values as follows:\n",
    "\n",
    "\n",
    "- $p_1 = 10/n$, $p_2 = 20/n$, $p_3= 1 - p_1 - p_2$\n",
    "- $\\mu_1 = \\frac{1}{10} \\sum_{i=1}^{10} x_i$,  the mean of the first 10 samples; $\\mu_2 = \\frac{1}{20} \\sum_{i=11}^{30} x_i$, the mean of next 20 samples; and $\\mu_3$ = the mean of the remaining samples. \n",
    "- Calculate $\\Sigma$ as \n",
    "$$\n",
    "\\frac{1}{n} \\Big [ \\sum_{i=1}^{10} (x_i- \\mu_1)(x_i- \\mu_1)^t + \\sum_{i=11}^{30} (x_i- \\mu_2)(x_i- \\mu_2)^t + \\sum_{i=31}^n (x_i- \\mu_3)(x_i- \\mu_3)^t \\Big].$$\n",
    "\n",
    "\n",
    "Run your EM implementation with **20** iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(data)\n",
    "G = 3\n",
    "ndim = 2\n",
    "prob = np.array([10/n, 20/n, 1-10/n-20/n])\n",
    "mu = np.array([np.mean(data[:10], axis=0), np.mean(data[10:30], axis=0),\n",
    "np.mean(data[30:], axis=0)])\n",
    "mu = mu.T\n",
    "Sigma = np.zeros((ndim,ndim))\n",
    "temp_data1 = np.array(data[:10]-mu[:, 0])\n",
    "temp_data2 = np.array(data[10:30]-mu[:, 1])\n",
    "temp_data3 = np.array(data[30:]-mu[:, 2])\n",
    "Sigma[0, 0] = np.sum(np.multiply(temp_data1[:,0], temp_data1[:,0])) + np.sum(np.multiply(temp_data2[:,0], temp_data2[:,0])) + np.sum(np.multiply(temp_data3[:,0], temp_data3[:,0]))\n",
    "Sigma[0, 1] = Sigma[1, 0] = np.sum(np.multiply(temp_data1[:,0],temp_data1[:,1]))+np.sum(np.multiply(temp_data2[:,0],temp_data2[:,1]))+np.sum(np.multiply(temp_data3[:,0],temp_data3[:,1]))\n",
    "Sigma[1, 1] = np.sum(np.multiply(temp_data1[:,1], temp_data1[:,1])) + np.sum(np.multiply(temp_data2[:,1], temp_data2[:,1])) + np.sum(np.multiply(temp_data3[:,1], temp_data3[:,1]))\n",
    "Sigma /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prob, new_mu, new_Sigma, llh = myEM(data, G, prob, mu, Sigma, itmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob\n",
      "[0.04363422 0.07718656 0.87917922]\n",
      "mean\n",
      "[[ 3.51006918  2.81616674  3.54564083]\n",
      " [77.10563811 63.35752634 71.25084801]]\n",
      "Sigma\n",
      "           eruptions     waiting\n",
      "eruptions   1.260158   13.511538\n",
      "waiting    13.511538  177.964191\n",
      "loglik\n",
      "-1289.3509588627353\n"
     ]
    }
   ],
   "source": [
    "print(\"prob\")\n",
    "print(new_prob)\n",
    "print(\"mean\")\n",
    "print(new_mu)\n",
    "print(\"Sigma\")\n",
    "print(new_Sigma)\n",
    "print(\"loglik\")\n",
    "print(llh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: HMM\n",
    "**Objective**\n",
    "Implement the `Baum-Welch` (i.e., EM) algorithm and the `Viterbi` algorithm from scratch for a Hidden Markov Model (HMM) that produces an outcome sequence of discrete random variables with three distinct values.\n",
    "A quick review on parameters for Discrete HMM:\n",
    "\n",
    "- `mx`: Count of distinct values X can take.\n",
    "- `mz`: Count of distinct values Z can take.\n",
    "- `w`: An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "- `A`: The mz-by-mz transition probability matrix that models the progression from Zt to Zt+1.\n",
    "- `B`: The mz-by-mx emission probability matrix, indicating how X is produced from Z.\n",
    "\n",
    "Focus on updating the parameters `A` and `B` in your algorithm. The value for `mx` is given and you’ll specify `mz`.\n",
    "\n",
    "For `w`, initiate it uniformly but refrain from updating it within your code. The reason for this is that `w` denotes the distribution of Z1 and we only have a single sample. It’s analogous to estimating the likelihood of a coin toss resulting in heads by only tossing it once. Given the scant information and the minimal influence on the estimation of other parameters, we can skip updating it.\n",
    "\n",
    "**Baum-Welch Algorihtm**\n",
    "The Baum-Welch Algorihtm is the EM algorithm for the HMM. Create a function named `BW.onestep` designed to carry out the E-step and M-step. This function should then be called iteratively within `myBW`.\n",
    "\n",
    "`BW.onstep`:\n",
    "- **Input**:\n",
    "    – data: a T-by-1 sequence of observations\n",
    "    – Current parameter values\n",
    "- **Output**:\n",
    "    – Updated parameters: A and B\n",
    "Please refer to formulas provided on Pages 7, 10, 14-16 in [lec_W7.2_HMM](https://liangfgithub.github.io/Notes/lec_W7.2_HMM.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sequence from the file\n",
    "# X: i.e., states,  start from 0 in python\n",
    "X = np.genfromtxt(\"coding4_part2_data.txt\", dtype=int) - 1\n",
    "w = np.array([0.5, 0.5])\n",
    "A = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "B = np.array([[1.0/9.0, 3.0/9.0, 5.0/9.0], [1.0/6.0, 2.0/6.0, 3.0/6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute alphas\n",
    "def compute_forward_probability(A, B, X, w):\n",
    "    T = len(X)  # Get the length of the sequence X\n",
    "    mz = A.shape[0]\n",
    "    # Initialize the forward probability matrix\n",
    "    alpha = np.zeros((T, mz))\n",
    "    \n",
    "    # Calculate the forward probabilities recursively\n",
    "    for t in range(T):\n",
    "        if t == 0:\n",
    "            # At t = 1, use the initial distribution w and emission probability B\n",
    "            alpha[t, :] = w * B[:, X[t]]\n",
    "            #print(f\" t: {t} , alpha[t] : {alpha[t] }\")\n",
    "        else:\n",
    "            # For t > 1, calculate alpha using the previous time step's alpha, transition probability A, and emission probability B\n",
    "            for i in range(mz):\n",
    "                # Using .dot to replace summation over j\n",
    "                alpha[t, i] = (np.dot(alpha[t - 1, :], A[:, i])) * B[i, X[t]]\n",
    "                #print(f\" t: {t} , i:{i}, alpha[{t},{i}] : {alpha[t,i] }\")\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betas\n",
    "def compute_backward_probability(A, B, X):\n",
    "    \n",
    "    T = len(X)\n",
    "    mz = A.shape[0]\n",
    "    beta = np.zeros((T, mz))\n",
    "\n",
    "    # Iterate backwards\n",
    "    for t in range(T-1,-1,-1):\n",
    "        if t == T-1:\n",
    "            # Last beta equals ones\n",
    "            beta[t, :] = np.ones(mz)\n",
    "        else:    \n",
    "            for i in range(mz):\n",
    "                # Using .dot to replace summation over j\n",
    "                beta[t, i] = np.dot(beta[t+1, :]*B[:,X[t+1]], A[i,:])\n",
    " \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baum-Welch Algorithm One Step\n",
    "def BW_one_step(A, B, X, w):\n",
    "    \n",
    "    T = len(X)\n",
    "    mz = A.shape[0]\n",
    "    mx = B.shape[1]\n",
    "    myGamma = np.zeros((T-1, mz, mz))\n",
    "    gamma_t = np.zeros((T, mz))    \n",
    "        \n",
    "    alpha = compute_forward_probability(A, B, X, w)\n",
    "    beta =  compute_backward_probability(A, B, X)   \n",
    "\n",
    "    # To update gamma_t, first calculate gammas on Slide#16    \n",
    "    for i in range(mz):\n",
    "        for j in range(mz):\n",
    "            for t in range(T-1):\n",
    "                myGamma[t,i,j] = alpha[t,i]*A[i,j]*B[j,X[t+1]]*beta[t+1,j]\n",
    "    \n",
    "    # update gamma_t based on gamma\n",
    "    for i in range(mz):\n",
    "        for t in range(T-1):\n",
    "            gamma_t[t,i] = np.sum(myGamma[t,i,:])\n",
    "\n",
    "    gamma_t[T-1, :] = gamma_t[T-2, :]\n",
    "    \n",
    "    # Update A_{mz x mz}\n",
    "    for i in range(mz):\n",
    "        for j in range(mz):\n",
    "            A[i,j] = np.sum(myGamma[:,i,j])\n",
    "        \n",
    "        A[i,:] = A[i,:]/np.sum(A[i, :])\n",
    "        \n",
    "    # Update B_{mz x mz}\n",
    "    for i in range(mz):\n",
    "        for l in range(mx):\n",
    "            B[i,l] = 0\n",
    "            for t in range(T):\n",
    "                B[i,l] += (X[t]==l)*(gamma_t[t,i])\n",
    "        B[i,:] = B[i,:]/np.sum(B[i, :])\n",
    "    \n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix after BW one step is\n",
      "[[0.485465   0.514535  ]\n",
      " [0.48503179 0.51496821]]\n",
      "B matrix after BW one step is\n",
      "[[0.2348986  0.19574883 0.56935256]\n",
      " [0.33224256 0.1845792  0.48317824]]\n"
     ]
    }
   ],
   "source": [
    "A1, B1 = BW_one_step(A, B, X, w)\n",
    "print(f'A matrix after BW one step is')\n",
    "print(A1)\n",
    "\n",
    "print(f'B matrix after BW one step is')\n",
    "print(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baum-Welch Algorithm multiple steps\n",
    "def myBW(Ai, Bi, X, w, n_iter):\n",
    "    A = Ai.copy()\n",
    "    B = Bi.copy()\n",
    "    for iter in range(n_iter):\n",
    "        A, B = BW_one_step(A, B, X, w)\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix after BW 100th step is\n",
      "[[0.49756056 0.50243944]\n",
      " [0.44840436 0.55159564]]\n",
      "B matrix after BW 100th step is\n",
      "[[0.22122219 0.20345148 0.57532633]\n",
      " [0.34199559 0.17797897 0.48002545]]\n"
     ]
    }
   ],
   "source": [
    "A100, B100 = myBW(A, B, X, w, 100)\n",
    "print(f'A matrix after BW 100th step is')\n",
    "print(A100)\n",
    "\n",
    "print(f'B matrix after BW 100th step is')\n",
    "print(B100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III Viterbi Algorithm\n",
    "This algorithm outputs the most likely latent sequence considering the data and the MLE of the parameters.\n",
    "`myViterbi`:\n",
    "- **Input**:\n",
    "\n",
    "    – data: a T-by-1 sequence of observations\n",
    "    \n",
    "    – parameters: `mx`, `mz`, `w`, `A` and `B`:\n",
    "    - `mx`: Count of distinct values X can take.\n",
    "    - `mz`: Count of distinct values Z can take.\n",
    "    -  `w`: An mz-by-1 probability vector representing the initial distribution for Z1.\n",
    "    - `A`: The mz-by-mz transition probability matrix that models the progression from Zt to Zt+1.\n",
    "    - `B`: The mz-by-mx emission probability matrix, indicating how X is produced from Z.\n",
    "\n",
    "- **Output**:\n",
    "\n",
    "    – Z: A T-by-1 sequence where each entry is a number ranging from 1 to mz.\n",
    "\n",
    "\n",
    "#### Note on Calculations in Viterbi:\n",
    "\n",
    "Many computations in HMM are based on the product of a sequence of probabilities, resulting in extremely small values. At times, these values are so small that software like R or Python might interpret them as zeros. This poses a challenge, especially for the Viterbi algorithm, where differentiating between magnitudes is crucial. If truncated to zero, making such distinctions becomes impossible. Therefore, it’s advisable to evaluate these probabilities on a logarithmic scale in the Viterbi algorithm.\n",
    "\n",
    "\n",
    "#### Testing\n",
    "Test your code with the provided data sequence: [Coding4_part2_data.txt]. Set `mz = 2` and start with the following initial values\n",
    "$$\n",
    "    w = \\begin{pmatrix}     \n",
    "        0.5 \\\\\n",
    "        0.5 \n",
    "        \\end{pmatrix}\n",
    "\n",
    "\n",
    "    A = \\begin{pmatrix}     \n",
    "        0.5 & 0.5\\\\\n",
    "        0.5 & 0.5\n",
    "        \\end{pmatrix} \n",
    "\n",
    "\n",
    "    B = \\begin{pmatrix}     \n",
    "        1/9 & 3/9 & 5/9\\\\\n",
    "        1/6 & 2/6 & 3/6\n",
    "        \\end{pmatrix}        \n",
    "$$,\n",
    "Run your implementation with **100** iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(data, A, B, w,  mx=3, mz=2):\n",
    "    T = len(data)\n",
    "    delta = np.zeros((T, mz))\n",
    "\n",
    "    log_A = np.log(A)\n",
    "    log_B = np.log(B)\n",
    "    log_w = np.log(w)\n",
    "\n",
    "\n",
    "    # Compute delta\n",
    "    delta[0,:] = log_w  +  log_B[:, data[0]]\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for i in range(mz):\n",
    "            # -1 because python index starts from 0\n",
    "            delta[t,i] = np.max(delta[t-1,:] + log_A[:,i]) + log_B[i, data[t]-1]\n",
    "\n",
    "    # compute the most prob sequence Z\n",
    "    Z = np.zeros(T).astype(int)\n",
    "\n",
    "    # start from the end\n",
    "\n",
    "    Z[T-1] = np.argmax(delta[T-1, :])\n",
    "\n",
    "    for t in range(T-2, -1, -1):\n",
    "        Z[t] = np.argmax(delta[t, :] + log_A[:, Z[t+1]])    \n",
    "\n",
    "    # +1: because python index start from 0\n",
    "    return Z + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "data = np.squeeze(pd.read_csv('coding4_part2_data.txt', header=None).values)\n",
    "\n",
    "A_bw = np.array([[0.49793938, 0.50206062],\n",
    "            [0.44883431, 0.55116569]])\n",
    "\n",
    "B_bw = np.array([[0.22159897, 0.20266127, 0.57573976],\n",
    "               [0.34175148, 0.17866665, 0.47958186]])\n",
    "\n",
    "w = np.array([0.5, 0.5])\n",
    "# Estimated states\n",
    "Z_est = viterbi(data, A_bw, B_bw, w)\n",
    "print(Z_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Actual states\n",
    "Z_act_list = np.squeeze(pd.read_csv('Coding4_part2_Z.txt',header=None).values)\n",
    "Z_act_str_list = []\n",
    "for z_i in Z_act_list:\n",
    "    z_i_str_list = z_i.strip().split(\" \")\n",
    "    Z_act_str_list.extend(z_i_str_list)\n",
    "\n",
    "Z_act = np.array([int(z) for z in Z_act_str_list])\n",
    "print(Z_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# The estimated states match the actual ones\n",
    "print((Z_act == Z_est).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
