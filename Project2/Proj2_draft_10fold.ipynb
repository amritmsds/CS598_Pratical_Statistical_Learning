{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train_data_with_year_week(dataframe: pd.DataFrame):\n",
    "    new_df = dataframe.copy()\n",
    "    new_df['Year'] = new_df['Date'].dt.isocalendar().year\n",
    "    new_df['Week'] = new_df['Date'].dt.isocalendar().week\n",
    "    new_df.drop(columns=['Date','IsHoliday'], inplace=True)\n",
    "    \n",
    "    # combine year and week for svd smoothing\n",
    "    new_df['Year_Week'] = new_df['Year'] * 100 + new_df['Week']\n",
    "    new_df.drop(columns=['Year','Week'], inplace=True)\n",
    "\n",
    "    # Re-order column names\n",
    "    return new_df[['Store','Dept','Year_Week','Weekly_Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_data_with_year_week(dataframe: pd.DataFrame):\n",
    "    new_df = dataframe.copy()\n",
    "    new_df['Year'] = new_df['Date'].dt.isocalendar().year\n",
    "    new_df['Week'] = new_df['Date'].dt.isocalendar().week\n",
    "    new_df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "    # Re-order column names\n",
    "    return new_df[['Store','Dept', 'Year', 'Week', 'IsHoliday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data(dateframe: pd.DataFrame,\n",
    "               keys: list= ['Store', 'Dept']):\n",
    "    df_grouped = dateframe.groupby(keys)\n",
    "    group_ids = []\n",
    "    groups = []\n",
    "    for id in df_grouped.groups:\n",
    "        group_ids.append(id)\n",
    "        groups.append(df_grouped.get_group(id).drop(columns=keys))\n",
    "    \n",
    "    return group_ids, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_train_data_with_svd(dataframe: pd.DataFrame,\n",
    "                                  num_pc: int = 8):\n",
    "    ## fill out missing values/clean outliers\n",
    "    group_ids, groups = group_data(dataframe, keys=['Dept'])\n",
    "    \n",
    "    svd_smoothed_df_list = []\n",
    "    for i in range(len(group_ids)):    \n",
    "        df_i = groups[i].pivot(index='Store', columns='Year_Week', values='Weekly_Sales').fillna(0)\n",
    "        mean_i = df_i.mean(axis=1).values\n",
    "        df_i_values_centered = (df_i.values.T - mean_i).T\n",
    "        U, S, Vh = np.linalg.svd(df_i_values_centered, full_matrices=False)\n",
    "        new_S = np.diag(S[:num_pc])\n",
    "\n",
    "        df_svd_smoothed_values = ((U[:,:num_pc]@new_S@Vh[:num_pc, :]).T + mean_i).T\n",
    "        \n",
    "\n",
    "        df_svd_smoothed = pd.DataFrame(df_svd_smoothed_values,\n",
    "                                       index=df_i.index,\n",
    "                                       columns=df_i.columns).reset_index()\n",
    "\n",
    "        df_i_svd_smoothed_unpivot = pd.melt(df_svd_smoothed, \n",
    "                                            id_vars='Store',\n",
    "                                            value_vars=df_svd_smoothed.columns).\\\n",
    "                                    sort_values(by=['Store','Year_Week'])\n",
    "        \n",
    "        df_i_svd_smoothed_unpivot['Year'] = (df_i_svd_smoothed_unpivot['Year_Week'].values//100).astype(int)\n",
    "        df_i_svd_smoothed_unpivot['Week'] = (df_i_svd_smoothed_unpivot['Year_Week'].values%100).astype(int)\n",
    "        df_i_svd_smoothed_unpivot.rename(columns={'value':'Weekly_Sales'}, inplace=True)\n",
    "        df_i_svd_smoothed_unpivot.drop(columns=['Year_Week'], inplace=True)\n",
    "        df_i_svd_smoothed_unpivot.reset_index(drop=True)\n",
    "        df_i_svd_smoothed_unpivot['Dept'] = group_ids[i]\n",
    "        svd_smoothed_df_list.append(df_i_svd_smoothed_unpivot)\n",
    "\n",
    "    \n",
    "    svd_smoothed_df = pd.concat(svd_smoothed_df_list).reset_index(drop=True)\n",
    "    holiday_weeks = [6, 36, 47, 52]\n",
    "    svd_smoothed_df['IsHoliday'] = np.where(svd_smoothed_df['Week'].isin(holiday_weeks), True, False)\n",
    "    \n",
    "    # Keep column order consistent with original one\n",
    "    return svd_smoothed_df[['Store', 'Dept','Year', 'Week', \"IsHoliday\", 'Weekly_Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_variable_transform(train_df, test_df):\n",
    "    # IMPORTANT:\n",
    "    # The test_dataframe needs to use the encoder from the trainng_dataframe, because some categories might be\n",
    "    # missing in the test data\n",
    "    categorical_feature_set = [feature for feature in train_df.columns if train_df[feature].dtypes=='object']\n",
    "    new_train_df = train_df.copy()\n",
    "    new_test_df = test_df.copy()\n",
    "    for feature in categorical_feature_set:\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        train_category_matrix = [[element] for element in train_df[feature]]\n",
    "        test_category_matrix = [[element] for element in test_df[feature]]\n",
    "\n",
    "        encoder.fit(train_category_matrix)\n",
    "        train_df_hot_code = pd.DataFrame(encoder.transform(train_category_matrix).toarray())\n",
    "        test_df_hot_code = pd.DataFrame(encoder.transform(test_category_matrix).toarray())\n",
    "\n",
    "        # Different from Project#1, add 1 here\n",
    "        train_df_hot_code.columns = [feature + '_' + str(c+1) for c in train_df_hot_code.columns]\n",
    "        test_df_hot_code.columns = [feature + '_' + str(c+1) for c in test_df_hot_code.columns]\n",
    "\n",
    "\n",
    "        # Replace the original feature with one-hot encoded feature\n",
    "        new_train_df.drop(columns=feature, inplace=True)\n",
    "        new_train_df = pd.concat([new_train_df, train_df_hot_code], axis=1)\n",
    "        \n",
    "        new_test_df.drop(columns=feature, inplace=True)\n",
    "        new_test_df = pd.concat([new_test_df, test_df_hot_code], axis=1)\n",
    "\n",
    "    # Further feature engineering\n",
    "    new_train_df.drop(columns='Weekly_Sales', inplace=True)\n",
    "    new_train_df['Year'] = new_train_df['Year'] - 2010 \n",
    "\n",
    "    new_test_df['Year'] = new_test_df['Year'] - 2010\n",
    "\n",
    "    return new_train_df, new_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df_train_svd_smoothed, df_train_one_hot_encode, df_test_one_hot_encode, alpha=5):\n",
    "    train_Y_group_ids, train_Y_groups = group_data(df_train_svd_smoothed[['Store', 'Dept', 'Weekly_Sales']])\n",
    "    train_X_group_ids, train_X_groups = group_data(df_train_one_hot_encode, keys= ['Store', 'Dept'])\n",
    "    test_X_group_ids, test_X_groups = group_data(df_test_one_hot_encode, keys=['Store', 'Dept'])\n",
    "    \n",
    "    prediction_results = []\n",
    "    for i, index_pair in enumerate(test_X_group_ids):\n",
    "        if index_pair not in train_X_group_ids:\n",
    "            pred_test = np.zeros(len(test_X_groups[i]))\n",
    "        else:\n",
    "            i_train = train_X_group_ids.index(index_pair)\n",
    "\n",
    "            temp_train_X = train_X_groups[i_train]\n",
    "            temp_train_Y = train_Y_groups[i_train]\n",
    "            temp_test_X = test_X_groups[i]\n",
    "\n",
    "            # The training X are basically 1 and 0 s, no need to stanardize\n",
    "            lasso_model = Lasso(alpha = alpha)\n",
    "            lasso_model.fit(temp_train_X, temp_train_Y)\n",
    "            pred_test = lasso_model.predict(temp_test_X)\n",
    "\n",
    "        prediction_results.extend(list(pred_test))\n",
    "    return np.array(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df_test, df_test_with_label):\n",
    "    scoring_df = df_test.drop(columns=['IsHoliday']).merge(df_test_with_label, on=['Store', 'Dept','Date'], how='left')\n",
    "    weights = scoring_df['IsHoliday'].apply(lambda x:5 if x else 1)\n",
    "    actuals = scoring_df['Weekly_Sales']\n",
    "    preds = scoring_df['Weekly_Pred']\n",
    "\n",
    "    wae = np.sum(weights * np.abs(actuals - preds)) / np.sum(weights)\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_10_fold(alpha=5):\n",
    "    project_path = \"Proj2_Data\"\n",
    "    df_test_with_label = pd.read_csv(os.path.join(project_path, \"test_with_label.csv\"), parse_dates=['Date'])\n",
    "    wae_list= []\n",
    "\n",
    "    for fold_num in range(1, 11):\n",
    "        fold = \"fold_\" + str(fold_num)\n",
    "        df_train = pd.read_csv(os.path.join(project_path, fold ,\"train.csv\"), parse_dates=['Date'])\n",
    "        df_test = pd.read_csv(os.path.join(project_path, fold, \"test.csv\"), parse_dates=['Date'])\n",
    "\n",
    "        df_train_transformed = transform_train_data_with_year_week(df_train)\n",
    "        df_test_transform = transform_test_data_with_year_week(df_test)\n",
    "\n",
    "        df_train_svd_smoothed =  smoothing_train_data_with_svd(dataframe=df_train_transformed)\n",
    "\n",
    "        df_train_svd_smoothed['Week'] = df_train_svd_smoothed['Week'].astype('object')\n",
    "        df_train_svd_smoothed['IsHoliday'] = df_train_svd_smoothed['IsHoliday'].astype('object')\n",
    "        df_train_one_hot_encode, df_test_one_hot_encode = categorical_variable_transform(df_train_svd_smoothed, df_test_transform)\n",
    "\n",
    "        df_test['Weekly_Pred'] = predict(df_train_svd_smoothed, df_train_one_hot_encode, df_test_one_hot_encode, alpha)\n",
    "        wae = evaluate(df_test, df_test_with_label)\n",
    "        wae_list.append(wae)\n",
    "    return wae_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanyan/miniforge3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.090e+06, tolerance: 1.116e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/hanyan/miniforge3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/hanyan/miniforge3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.950e+06, tolerance: 1.340e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/hanyan/miniforge3/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.849e+06, tolerance: 1.588e+06\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1848.612034776927, 1338.3572757170118, 1373.0654860858126, 1480.5823280611087, 2274.3635933298606, 1621.9928323374656, 1671.7658402169088, 1381.533476550177, 1400.9252820453414, 1405.457492699939]\n",
      "1579.6655641820555\n"
     ]
    }
   ],
   "source": [
    "wae_list = evaluate_10_fold()\n",
    "print(wae_list)\n",
    "print(sum(wae_list)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
