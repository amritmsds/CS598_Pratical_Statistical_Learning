{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prject_path = \"/Users/richardxu/Dropbox/UIUC_CS598_Statistical_Learning/Project2/Proj2_Data\"\n",
    "fold_num = 1\n",
    "fold = \"fold_\" + str(fold_num)\n",
    "df_train = pd.read_csv(os.path.join(prject_path, fold ,\"train.csv\"), parse_dates=['Date'])\n",
    "df_test = pd.read_csv(os.path.join(prject_path, fold, \"test.csv\"), parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: transform Date to Year/Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train_data_with_year_week(dataframe: pd.DataFrame):\n",
    "    new_df = dataframe.copy()\n",
    "    new_df['Year'] = new_df['Date'].dt.isocalendar().year\n",
    "    new_df['Week'] = new_df['Date'].dt.isocalendar().week\n",
    "    new_df.drop(columns=['Date','IsHoliday'], inplace=True)\n",
    "    \n",
    "    # combine year and week for svd smoothing\n",
    "    new_df['Year_Week'] = new_df['Year'] * 100 + new_df['Week']\n",
    "    new_df.drop(columns=['Year','Week'], inplace=True)\n",
    "\n",
    "    # Re-order column names\n",
    "    return new_df[['Store','Dept','Year_Week','Weekly_Sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test_data_with_year_week(dataframe: pd.DataFrame):\n",
    "    new_df = dataframe.copy()\n",
    "    new_df['Year'] = new_df['Date'].dt.isocalendar().year\n",
    "    new_df['Week'] = new_df['Date'].dt.isocalendar().week\n",
    "    new_df.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "    # Re-order column names\n",
    "    return new_df[['Store','Dept', 'Year', 'Week', 'IsHoliday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transformed = transform_train_data_with_year_week(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_transform = transform_test_data_with_year_week(df_test)\n",
    "#df_test_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Group data based on (Store, Dept) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data(dateframe: pd.DataFrame,\n",
    "               keys: list= ['Store', 'Dept']):\n",
    "    df_grouped = dateframe.groupby(keys)\n",
    "    group_ids = []\n",
    "    groups = []\n",
    "    for id in df_grouped.groups:\n",
    "        group_ids.append(id)\n",
    "        groups.append(df_grouped.get_group(id).drop(columns=keys))\n",
    "    \n",
    "    return group_ids, groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: SVD smoothing on individual groups, remove outliers/noise, filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_train_data_with_svd(dataframe: pd.DataFrame,\n",
    "                                  num_pc: int = 8):\n",
    "    ## fill out missing values/clean outliers\n",
    "    group_ids, groups = group_data(dataframe, keys=['Dept'])\n",
    "    \n",
    "    svd_smoothed_df_list = []\n",
    "    for i in range(len(group_ids)):    \n",
    "        df_i = groups[i].pivot(index='Store', columns='Year_Week', values='Weekly_Sales').fillna(0)\n",
    "        mean_i = df_i.mean(axis=1).values\n",
    "        df_i_values_centered = (df_i.values.T - mean_i).T\n",
    "        U, S, Vh = np.linalg.svd(df_i_values_centered, full_matrices=False)\n",
    "        new_S = np.diag(S[:num_pc])\n",
    "\n",
    "        df_svd_smoothed_values = ((U[:,:num_pc]@new_S@Vh[:num_pc, :]).T + mean_i).T\n",
    "        \n",
    "\n",
    "        df_svd_smoothed = pd.DataFrame(df_svd_smoothed_values,\n",
    "                                       index=df_i.index,\n",
    "                                       columns=df_i.columns).reset_index()\n",
    "\n",
    "        df_i_svd_smoothed_unpivot = pd.melt(df_svd_smoothed, \n",
    "                                            id_vars='Store',\n",
    "                                            value_vars=df_svd_smoothed.columns).\\\n",
    "                                    sort_values(by=['Store','Year_Week'])\n",
    "        \n",
    "        df_i_svd_smoothed_unpivot['Year'] = (df_i_svd_smoothed_unpivot['Year_Week'].values//100).astype(int)\n",
    "        df_i_svd_smoothed_unpivot['Week'] = (df_i_svd_smoothed_unpivot['Year_Week'].values%100).astype(int)\n",
    "        df_i_svd_smoothed_unpivot.rename(columns={'value':'Weekly_Sales'}, inplace=True)\n",
    "        df_i_svd_smoothed_unpivot.drop(columns=['Year_Week'], inplace=True)\n",
    "        df_i_svd_smoothed_unpivot.reset_index(drop=True)\n",
    "        df_i_svd_smoothed_unpivot['Dept'] = group_ids[i]\n",
    "        svd_smoothed_df_list.append(df_i_svd_smoothed_unpivot)\n",
    "\n",
    "    \n",
    "    svd_smoothed_df = pd.concat(svd_smoothed_df_list).reset_index(drop=True)\n",
    "    holiday_weeks = [6, 36, 47, 52]\n",
    "    svd_smoothed_df['IsHoliday'] = np.where(svd_smoothed_df['Week'].isin(holiday_weeks), True, False)\n",
    "    \n",
    "    # Keep column order consistent with original one\n",
    "    return svd_smoothed_df[['Store', 'Dept','Year', 'Week', \"IsHoliday\", 'Weekly_Sales']]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_svd_smoothed =  smoothing_train_data_with_svd(dataframe=df_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_svd_smoothed['Week'] = df_train_svd_smoothed['Week'].astype('object')\n",
    "df_train_svd_smoothed['IsHoliday'] = df_train_svd_smoothed['IsHoliday'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_variable_transform(train_df, test_df):\n",
    "    # IMPORTANT:\n",
    "    # The test_dataframe needs to use the encoder from the trainng_dataframe, because some categories might be\n",
    "    # missing in the test data\n",
    "    \n",
    "    categorical_feature_set = [feature for feature in train_df.columns if train_df[feature].dtypes=='object']\n",
    "    new_train_df = train_df.copy()\n",
    "    new_test_df = test_df.copy()\n",
    "    for feature in categorical_feature_set:\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        train_category_matrix = [[element] for element in train_df[feature]]\n",
    "        test_category_matrix = [[element] for element in test_df[feature]]\n",
    "\n",
    "        encoder.fit(train_category_matrix)\n",
    "        train_df_hot_code = pd.DataFrame(encoder.transform(train_category_matrix).toarray())\n",
    "        test_df_hot_code = pd.DataFrame(encoder.transform(test_category_matrix).toarray())\n",
    "\n",
    "        # Different from Project#1, add 1 here\n",
    "        train_df_hot_code.columns = [feature + '_' + str(c+1) for c in train_df_hot_code.columns]\n",
    "        test_df_hot_code.columns = [feature + '_' + str(c+1) for c in test_df_hot_code.columns]\n",
    "\n",
    "\n",
    "        # Replace the original feature with one-hot encoded feature\n",
    "        new_train_df.drop(columns=feature, inplace=True)\n",
    "        new_train_df = pd.concat([new_train_df, train_df_hot_code], axis=1)\n",
    "        \n",
    "        new_test_df.drop(columns=feature, inplace=True)\n",
    "        new_test_df = pd.concat([new_test_df, test_df_hot_code], axis=1)\n",
    "\n",
    "    # Further feature engineering\n",
    "    new_train_df.drop(columns='Weekly_Sales', inplace=True)\n",
    "    new_train_df['Year'] = new_train_df['Year'] - 2010 \n",
    "\n",
    "    new_test_df['Year'] = new_test_df['Year'] - 2010\n",
    "\n",
    "    return new_train_df, new_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_one_hot_encode, df_test_one_hot_encode = categorical_variable_transform(df_train_svd_smoothed, df_test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Build Lasso regression models and make predictions for individual groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y : Weekly_Sales\n",
    "# Each group has weekly_sales associated with a (Store, Dept) pair\n",
    "train_Y_group_ids, train_Y_groups = group_data(df_train_svd_smoothed[['Store', 'Dept', 'Weekly_Sales']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by (Store, Dept) pair\n",
    "train_X_group_ids, train_X_groups = group_data(df_train_one_hot_encode, keys= ['Store', 'Dept'])\n",
    "test_X_group_ids, test_X_groups = group_data(df_test_one_hot_encode, keys=['Store', 'Dept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardxu/opt/anaconda3/envs/python39/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "prediction_results = []\n",
    "for i, index_pair in enumerate(test_X_group_ids):\n",
    "    if index_pair not in train_X_group_ids:\n",
    "        #print(\"Oops, cannot find at ({}, {})\".format(index_pair[0], index_pair[1]))\n",
    "        pred_test = np.zeros(len(test_X_groups[i]))\n",
    "    else:\n",
    "        i_train = train_X_group_ids.index(index_pair)\n",
    "\n",
    "        temp_train_X = train_X_groups[i_train]\n",
    "        temp_train_Y = train_Y_groups[i_train]\n",
    "        temp_test_X = test_X_groups[i]\n",
    "        \n",
    "        # Not working as good as Lasso\n",
    "        #xgb_model = XGBRegressor(n_estimators=100,max_depth=2)\n",
    "        #xgb_pipeline = Pipeline(steps=[(\"scalar\",StandardScaler()), (\"xgb\", xgb_model)])\n",
    "        #xgb_pipeline.fit(temp_train_X, temp_train_Y)\n",
    "        #pred_test = xgb_pipeline.predict(temp_test_X)\n",
    "        \n",
    "\n",
    "        # The training X are basically 1 and 0 s, no need to stanardize\n",
    "\n",
    "        lasso_model = Lasso(alpha = 10)\n",
    "        #lasso_pipeline = Pipeline(steps=[(\"scalar\",StandardScaler()), (\"lasso\", lasso_model)])\n",
    "        #lasso_pipeline.fit(temp_train_X, temp_train_Y)\n",
    "        #pred_test = lasso_pipeline.predict(temp_test_X)\n",
    "        lasso_model.fit(temp_train_X, temp_train_Y)\n",
    "        pred_test = lasso_model.predict(temp_test_X)\n",
    "\n",
    "    prediction_results.extend(list(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Weekly_Pred'] = np.array(prediction_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_with_label = pd.read_csv(os.path.join(prject_path, \"test_with_label.csv\"), parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_df = df_test.drop(columns=['IsHoliday']).merge(df_test_with_label, on=['Store', 'Dept','Date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract weights and convert to numpy arrays for wae calculation\n",
    "weights = scoring_df['IsHoliday'].apply(lambda x:5 if x else 1)\n",
    "actuals = scoring_df['Weekly_Sales']\n",
    "preds = scoring_df['Weekly_Pred']\n",
    "\n",
    "wae = np.sum(weights * np.abs(actuals - preds)) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1820.752145237265"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manualy run across 10 folders, got an average of 1590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1590.508"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1820+1359.5+1406.4+1482.3+2290+1633.8+1678.6+1396.4+1418.08+1420)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
